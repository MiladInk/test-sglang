{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"MathMindsAGI/MATH-openai-split\")\n",
    "train_ds = dataset[\"train\"]\n",
    "val_ds = dataset[\"validation\"]\n",
    "test_ds = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.42s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.42s/it]\n",
      "\n",
      "100%|██████████| 23/23 [00:08<00:00,  2.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# start sglang offline server\n",
    "import sglang as sgl\n",
    "\n",
    "# llm = sgl.Engine(model_path=\"Qwen/Qwen2-1.5B-Instruct\")\n",
    "llm = sgl.Engine(model_path=\"Qwen/Qwen2-Math-1.5B-Instruct\")\n",
    "# llm = sgl.Engine(model_path=\"realtreetune/deepseekmath-7b-sft-MATH-v2\", dp_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is 2+2?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def llm_format_math_query(math_query): # as this is chat model, the actual formatting happens with the chat_template\n",
    "    return math_query\n",
    "    \n",
    "llm_format_math_query(\"What is 2+2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate response per prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_grader_minerva import eval_math\n",
    "from math_answer_extraction import extract_math_answer\n",
    "\n",
    "def is_correct(query, ground_truth_answer, response):\n",
    "    extracted_answer = extract_math_answer(query, response)\n",
    "    if extracted_answer is None or len(extracted_answer) == 0:\n",
    "        return False\n",
    "    return eval_math(item={'prediction': extracted_answer, 'answer': [ground_truth_answer]}, pred_key='prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pseudo code for generating responses - single gpu\n",
    "```python\n",
    "def generate_responses(queries_dataset):\n",
    "    \"\"\"\n",
    "    input: dataset(query)\n",
    "    output: dataset(query, resposne)\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_0: ['<|im_start|>', 'system', 'Ċ', 'You', 'Ġare', 'Ġa', 'Ġhelpful', 'Ġassistant', '.', '<|im_end|>', 'Ċ', '<|im_start|>', 'user', 'Ċ', 'What', 'Ġis', 'Ġ', '2', '+', '2', '?', '<|im_end|>', 'Ċ', '<|im_start|>', 'assistant', 'Ċ', 'The', 'Ġanswer', 'Ġis']\n",
      " tokens_1: ['<|im_start|>', 'system', 'Ċ', 'You', 'Ġare', 'Ġa', 'Ġhelpful', 'Ġassistant', '.', '<|im_end|>', 'Ċ', '<|im_start|>', 'user', 'Ċ', 'What', 'Ġis', 'Ġ', '2', '+', '2', '?', '<|im_end|>', 'Ċ', '<|im_start|>', 'assistant', 'Ċ', 'The', 'Ġanswer', 'Ġis']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nWhat is 2+2?\\nPlease reason step by step, and put your final answer within \\\\boxed{}.<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def test_chat_template_for_our_hypothesis(tokenizer):\n",
    "    \"\"\"\n",
    "    This test is to verify concatenating the partial response to the chat template output is the same as using continue_final_message=True.\n",
    "    \"\"\"\n",
    "    \n",
    "    partial_response = \"The answer is\"\n",
    "    messages_0 = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": 'What is 2+2?'},\n",
    "        {\"role\": \"assistant\", \"content\": partial_response},\n",
    "    ]\n",
    "    \n",
    "    messages_1 = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": 'What is 2+2?'},\n",
    "    ]\n",
    "    \n",
    "    text_0 = tokenizer.apply_chat_template(messages_0, tokenize=False, continue_final_message=True)\n",
    "    text_1 = tokenizer.apply_chat_template(messages_1, tokenize=False, add_generation_prompt=True) + partial_response\n",
    "    \n",
    "    tokens_0 = tokenizer.tokenize(text_0, add_special_tokens=False)\n",
    "    tokens_1 = tokenizer.tokenize(text_1, add_special_tokens=False)\n",
    "    \n",
    "    print(f\"tokens_0: {tokens_0}\\n tokens_1: {tokens_1}\")\n",
    "\n",
    "    assert tokens_0 == tokens_1, \"Oh oh! The tokens are not the same!\"\n",
    "    \n",
    "\n",
    "def llm_chat_template(tokenizer, query, assistant_partial_respone=''):\n",
    "    # you may wonder: why didn't we just use continue_final_message=True? Because when assistant_partial_response is '', the tokenizer adds the <im_end> token :/ idk why.\n",
    "    instruct_math_eval_prompt = \"{query}\\nPlease reason step by step, and put your final answer within \\\\boxed{{}}.\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": instruct_math_eval_prompt.format(query=query)},\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    chat_query = text + assistant_partial_respone\n",
    "    \n",
    "    return chat_query\n",
    "\n",
    "test_chat_template_for_our_hypothesis(tokenizer)\n",
    "llm_chat_template(tokenizer, query=\"What is 2+2?\", assistant_partial_respone='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(ds):\n",
    "    sampling_params = {\"temperature\": 0.6, \"top_p\": 0.9, \"max_new_tokens\": 512}\n",
    "    ds = ds.map(lambda x: {'query': llm_chat_template(tokenizer, query=x['problem'], assistant_partial_respone='')})\n",
    "    queries_input_ids= ds.map(lambda x: {'query_input_ids': tokenizer(x['query'], add_special_tokens=False)['input_ids']})['query_input_ids']\n",
    "    responses = llm.generate(input_ids=queries_input_ids, sampling_params=sampling_params) # can't use map here, it should be in one call and sglang handles the batching\n",
    "    ds = ds.add_column('response', responses)\n",
    "    return ds\n",
    "\n",
    "def generated_response_stats(ds):    \n",
    "    response_tokens_count = ds.map(lambda x: {'y': x['response']['meta_info']['completion_tokens']})['y']\n",
    "    avg_response_token = sum(response_tokens_count) / len(response_tokens_count)\n",
    "    all_response_tokens = sum(response_tokens_count)\n",
    "    \n",
    "    is_response_correct = ds.map(lambda x: {'y': is_correct(query=x['query'], ground_truth_answer=x['answer'], response=x['response']['text'])})['y']\n",
    "    avg_correct = sum(is_response_correct) / len(is_response_correct)\n",
    "    \n",
    "    return {\n",
    "        'avg_response_token': avg_response_token,\n",
    "        'avg_correct': avg_correct,\n",
    "        'all_response_tokens': all_response_tokens\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72ff498dc30455fa4e1989687d6ef52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431c9c438039480caddd2c9147ce1d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token throughput: 12631.39 tokens per second\n",
      "Average response token: 415.50\n",
      "Average correct: 0.58\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start_time = time.time()\n",
    "ds = generate_responses(train_ds.select(range(100)))\n",
    "end_time = time.time()\n",
    "stats = generated_response_stats(ds)\n",
    "token_throughput = stats['all_response_tokens'] / (end_time - start_time)\n",
    "print(f\"Token throughput: {token_throughput:.2f} tokens per second\")\n",
    "print(f\"Average response token: {stats['avg_response_token']:.2f}\")\n",
    "print(f\"Average correct: {stats['avg_correct']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8cb9dd0c654239b95ebc156e8d1b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd989c60b274d0aa925b331aca67456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709eba9f6eff4d87b8ddc3e3ef160e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from math_extract_steps_inplace import split_solution_inplace\n",
    "\n",
    "def get_prefixes(response, step_boundaries):\n",
    "        prefixes = ['',]\n",
    "        for i in range(len(step_boundaries) - 1):\n",
    "            prefixes.append(response[step_boundaries[i]:step_boundaries[i+1]])\n",
    "        return {'response_prefixes': prefixes}\n",
    "    \n",
    "def get_prefix_queries(query, prefixes):\n",
    "    prefix_queries = []\n",
    "    for prefix in prefixes:\n",
    "        prefix_queries.append(f\"{query}{prefix}\")\n",
    "    return {'prefix_queries': prefix_queries}\n",
    "    \n",
    "def get_step_values(query, ground_truth_answer, response_prefixes, prefixes_mcs_outputs):\n",
    "    step_values = []\n",
    "    for prefix, outputs in zip(response_prefixes, prefixes_mcs_outputs):\n",
    "        values = []\n",
    "        for output in outputs:\n",
    "            completion = output['text']\n",
    "            score = is_correct(query=query, ground_truth_answer=ground_truth_answer,response=f\"{prefix}{completion}\") # TODO: handle no <eos>\n",
    "            values.append(score) \n",
    "        step_values.append(sum(values) / len(values))\n",
    "    return {'step_values': step_values}\n",
    "    \n",
    "def estimate_step_values_by_MC(ds, K: int):\n",
    "    \"\"\"\n",
    "    input: dataset(query, step_boundaries)\n",
    "    output: dataset(query, step_boundaries, step_values)\n",
    "    \"\"\"\n",
    "    \n",
    "    ds = ds.map(lambda x: get_prefixes(x['response']['text'], x['step_boundaries']))\n",
    "    ds = ds.map(lambda x: get_prefix_queries(x['query'], x['response_prefixes']))\n",
    "    \n",
    "    # for efficiency, we don't submit thousands of individual requests.\n",
    "    # we will batch all requests and let sglang handle the scheduling for us.\n",
    "    # this makes the code a bit dirty to disenangle them afterwards though.\n",
    "    # we keep track of the counts of prefixes per item. and then split the outputs accordingly.\n",
    "    all_mc_queries = ds['prefix_queries']\n",
    "    flatten_mc_queries = []\n",
    "    count_queries = []\n",
    "    for mc_queries in all_mc_queries:\n",
    "        flatten_mc_queries.extend(mc_queries)\n",
    "        count_queries.append(len(mc_queries))\n",
    "    \n",
    "    sampling_params = {\"temperature\": 0.6, \"top_p\": 0.9, \"max_new_tokens\": 512, \"n\": K}\n",
    "    flatten_mc_queries_input_ids = tokenizer(flatten_mc_queries, add_special_tokens=False)['input_ids']\n",
    "    mc_outputs = llm.generate(input_ids=flatten_mc_queries_input_ids, sampling_params=sampling_params)\n",
    "    \n",
    "    # first, reshape from nxk to n list of k outputs (sglang just returns a flat list)\n",
    "    assert len(mc_outputs) == len(flatten_mc_queries) * K\n",
    "    outputs = []\n",
    "    for i in range(0, len(flatten_mc_queries)):\n",
    "        outputs.append(mc_outputs[i*K:i*K+K])\n",
    "        \n",
    "    # second, split the outputs per item\n",
    "    outputs_per_prefix = []\n",
    "    start = 0\n",
    "    for count in count_queries:\n",
    "        outputs_per_prefix.append(outputs[start:start+count])\n",
    "        start += count\n",
    "        \n",
    "    ds = ds.add_column('prefixes_mcs_output', outputs_per_prefix)\n",
    "    return ds \n",
    "\n",
    "def get_step_advantages(values):\n",
    "    advantages = [values[i] - values[i-1] for i in range(1, len(values))]\n",
    "    return {'step_advantages': advantages}\n",
    "\n",
    "def map_step_advantages_to_token_advantages(tokenizer, query, response, step_boundaries, step_advantages):\n",
    "    # test case for advantage computation\n",
    "    text = f\"{query}{response}\"\n",
    "    \n",
    "     # building the mask is a bit tricky, where does query tokens end and response tokens start? we assume they are tokenized separately.\n",
    "    assert tokenizer.encode(query, add_special_tokens=False) + tokenizer.encode(response, add_special_tokens=False) == tokenizer.encode(text, add_special_tokens=False), \"one token is both at the end of the query and the beginning of the response\"\n",
    "    # we will build the mask from the position of the last char of the query returned by offset_mapping, as we don't know if the tokenizer adds special tokens.\n",
    "    \n",
    "    assert step_boundaries[0] == 0\n",
    "    step_boundaries = [0] + [x + len(query) for x in step_boundaries[1:]] # step_boundries are relative to the response, we need to adjust them to the text, make query of step 0\n",
    "    # for i in range(len(step_boundaries)-1):\n",
    "    #     step_text = text[step_boundaries[i]:step_boundaries[i+1]]\n",
    "    #     print(f\"step {i}th:{step_text}\")\n",
    "\n",
    "    encoded = tokenizer(text,\n",
    "                        return_offsets_mapping=True,\n",
    "                        return_tensors=\"pt\")\n",
    "\n",
    "    offset_mapping = encoded['offset_mapping'][0]\n",
    "    input_ids = encoded['input_ids'][0]\n",
    "\n",
    "    # determine which step each token belongs to\n",
    "    # we know the [start_char_idx, end_char_idx) of each step. Using the offset_mapping, we can determine which step each token belongs to\n",
    "    # Although close to impossible a token can span two steps, we will assign the advantage of the first step it belongs to.\n",
    "    token_to_step = []\n",
    "    total_steps = len(step_boundaries) - 1\n",
    "    assert total_steps > 0\n",
    "    step_start_char_idx, step_end_char_idx = step_boundaries[0], step_boundaries[1] # [start_char_idx, end_char_idx)\n",
    "    step = 0\n",
    "    for token_start_char_idx, token_end_char_idx in offset_mapping.tolist(): # [start_idx of the char of this token, end_idx of the char of this token)\n",
    "        if token_start_char_idx < step_end_char_idx:\n",
    "            assert token_start_char_idx >= step_start_char_idx\n",
    "        else:\n",
    "            step += 1\n",
    "            assert step < total_steps\n",
    "            step_start_char_idx, step_end_char_idx = step_boundaries[step], step_boundaries[step+1]\n",
    "            assert token_start_char_idx < step_end_char_idx\n",
    "            assert token_start_char_idx >= step_start_char_idx, \"maybe a token is split across more than two steps, this is terrible\"\n",
    "        \n",
    "        token_to_step.append(step)\n",
    "        \n",
    "    # now, will find the position of the last token of the query\n",
    "    query_end_char_idx = len(query)-1\n",
    "    for i, (token_start_char_idx, token_end_char_idx) in enumerate(offset_mapping.tolist()):\n",
    "        if token_start_char_idx > query_end_char_idx:\n",
    "            query_end_token_idx = i-1\n",
    "            break\n",
    "    else:\n",
    "        query_end_token_idx = i\n",
    "    \n",
    "\n",
    "    assert step == total_steps - 1\n",
    "    assert len(token_to_step) == len(input_ids)\n",
    "\n",
    "    # print(f\"token_which_step: {token_to_step}\")\n",
    "\n",
    "    # assign the advantages to the tokens\n",
    "    token_advantages = [0] * len(input_ids) \n",
    "    for i in range(len(input_ids)):\n",
    "        token_advantages[i] = step_advantages[token_to_step[i]]\n",
    "    \n",
    "    # print alongside the tokens\n",
    "    # for (step, token_id, adv) in zip(token_to_step, input_ids, token_advantages):\n",
    "    #     print(f\" token: {tokenizer.decode(token_id)}, step: {step}, advantage: {adv}\")  \n",
    "    \n",
    "    query_tokens = query_end_token_idx + 1\n",
    "    response_tokens = len(input_ids) - query_tokens\n",
    "    mask = [0] * query_tokens + [1] * response_tokens\n",
    "    \n",
    "   \n",
    "    \n",
    "    return {'token_advantages': token_advantages, 'full_text': text, 'mask': mask, 'full_text_input_ids': input_ids}\n",
    "\n",
    "start_time = time.time()\n",
    "ds = ds.map(lambda x: {'step_boundaries': split_solution_inplace(x['response']['text'])})\n",
    "mc_ds = estimate_step_values_by_MC(ds, K=9)  \n",
    "mc_ds = mc_ds.map(lambda x: get_step_values(x['query'], x['answer'], x['response_prefixes'], x['prefixes_mcs_output']))  \n",
    "mc_ds = mc_ds.map(lambda x: get_step_advantages(x['step_values']))\n",
    "mc_ds = mc_ds.map(lambda x: map_step_advantages_to_token_advantages(llm.get_tokenizer(), x['query'], x['response']['text'], x['step_boundaries'], x['step_advantages']))\n",
    "mc_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a full_text vs token_advantages\n",
    "# row = mc_ds[1]\n",
    "# full_text = row['full_text']\n",
    "# mask = row['mask']\n",
    "# token_advantages = row['token_advantages']\n",
    "# for i, (token_id, adv) in enumerate(zip(row['full_text_input_ids'], token_advantages)):\n",
    "#     print(f\"token: {llm.get_tokenizer().decode(token_id)} \\t, advantage: {adv:.2f}, mask: {mask[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993c136a7ca24cb4b18271ebc6a2f220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683ecda7c1a445ef85c6b38fccd79d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44676a82276c47af93bd26f906eafb6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5cf2fadc56f4fa3ba698cf09f77456e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f00e2de85d44e8495c115f97754d238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e4f53e405c47ada101ef01a67c362a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_vine_episodes(ds):\n",
    "    \"\"\"\n",
    "    input: dataset['query', 'answer', 'response']\n",
    "    output: dataset['query', 'answer', 'response', 'full_text_input_ids', 'token_advantages', 'mask'], time taken\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    ds = ds.map(lambda x: {'step_boundaries': split_solution_inplace(x['response']['text'])})\n",
    "    mc_ds = estimate_step_values_by_MC(ds, K=9)  \n",
    "    mc_ds = mc_ds.map(lambda x: get_step_values(x['query'], x['answer'], x['response_prefixes'], x['prefixes_mcs_output']))  \n",
    "    mc_ds = mc_ds.map(lambda x: get_step_advantages(x['step_values']))\n",
    "    mc_ds = mc_ds.map(lambda x: map_step_advantages_to_token_advantages(llm.get_tokenizer(), x['query'], x['response']['text'], x['step_boundaries'], x['step_advantages']))\n",
    "    mc_time = time.time() - start_time\n",
    "    \n",
    "    return mc_ds, mc_time\n",
    "\n",
    "mc_ds, mc_time = generate_vine_episodes(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 660284\n",
      "Cached token count: 222309\n",
      "MC time: 27.79 seconds\n",
      "Token throughput: 23756.21 tokens per second\n",
      "Cached token throughput: 7998.41 tokens per second\n"
     ]
    }
   ],
   "source": [
    "# throughput analysis\n",
    "token_count = 0\n",
    "cached_token_count = 0\n",
    "for prefixes_mcs_output in mc_ds['prefixes_mcs_output']:\n",
    "    for prefix_mcs_output in prefixes_mcs_output:\n",
    "        for mcs_output in prefix_mcs_output:\n",
    "            token_count += mcs_output['meta_info']['completion_tokens']\n",
    "            cached_token_count += mcs_output['meta_info']['cached_tokens']\n",
    "            \n",
    "print(f\"Token count: {token_count}\")\n",
    "print(f\"Cached token count: {cached_token_count}\")\n",
    "print(f\"MC time: {mc_time:.2f} seconds\")\n",
    "print(f\"Token throughput: {token_count / mc_time:.2f} tokens per second\")\n",
    "print(f\"Cached token throughput: {cached_token_count / mc_time:.2f} tokens per second\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[12555, 374, 419, 30], [14990]], 'attention_mask': [[1, 1, 1, 1], [1]]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['what is this?',  'hello'], add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pseudo code for episode generator - single gpu\n",
    "```python\n",
    "def generate_episode(dataset):\n",
    "    \"\"\"\n",
    "    input: dataset(query, response)\n",
    "    output: episode(input_ids, advantages, mask)\n",
    "    \"\"\"\n",
    "    step_boundaries = split_into_steps(input)\n",
    "    step_values = compute_mc_values(input, step_boundaries)\n",
    "    step_advantages = ...\n",
    "    token_advantages = ...\n",
    "\n",
    "    mask = ...\n",
    "    input_ids = ... \n",
    "    return(input_ids, token_advantages, mask)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random pieces"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Fixed Modules (ipykernel)",
   "language": "python",
   "name": "python3_fixed_modules"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
